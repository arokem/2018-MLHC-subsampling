\section{Discussion and related work}

The application of DL in biomedical imaging is a promising avenue in current
research. Future developments in this field may lead to accurate
computer-assisted diagnosis systems with DL networks at their core.  
A major current impediment to these developments is the assumption that DL 
requires very large data-sets that are not available for many types of nascent 
imaging technologies, or in the case of diseases that are relatively uncommon.

In the present work, we investigated the feasibility of DL with relatively small
sample sizes. We focused on a prototype of computer-assisted diagnosis system
that can accurately discriminate optical coherence tomography (OCT) images from
retinae of patients with age-related macular degeneration AMD, relative to OCT
images from the retinae of healthy controls.

\subsection{How many images do we need to discriminate AMD from healthy retina?}

We found that training a DL network to perform at a high level of accuracy does
not require millions of images. Instead, close-to-maximal performance is
achieved with as few as approximately 20,000 images. Furthermore, the moderate
increase in accuracy from 64\% to 100\% of the data suggests that further
increase in data size would not result in much higher accuracy. This suggests
that $\sim$87\% accuracy is as high as possible with these data and this DL
algorithm. This limit on performance may be related to data quality; the partial
accuracy of the labels in the EMR: these data are heterogeneous and ultimately
depends on clinical decisions made by human observers, as well as the limited
signal-to-noise ratio of the images in capturing the image features that are
diagnostic. However, we do expect further performance improvements to come from
more elaborate algorithms that incorporate additional information, or make
better use of the information in the images, rather than only from more or
better data.

The relatively small number  of images required to train a DL network on this
classification task is surprising given the VGG16 network has as many as 138M
parameters \cite{Canziani2016-ps}. Indeed, previous literature using similar
networks (e.g. \cite{Krizhevsky2012-az, Simonyan2014-al}) used many millions of
training samples to reach high accuracy. The discrepancy between our findings
and the previous literature may stem from the differences between the use-case
we present here, and the common use-cass for DL in previous literature. The
assumption that many items from each class are required and that many millions
of separate images are needed to train DL algorithms stems from the object
classification literature mentioned in the introduction, but object
classification in natural images addresses several challenges that are not
typical  in the classification of medical images, and particularly clinical
images from OCT. Primary to these challenges is the variance in pose and
orientation of natural objects within photographic images, which leads to large
variance in the appearance of these objects. To capture all the variations of a
category (e.g., 'dog'), a DL network would have to be exposed to many thousands
of exemplars of this category, generalizing not over all the angles from which
this category could be captured, but also all the sub-categories of this
category (e.g., 'malamut' or 'poodle'). This variance in input is much more
limited in biomedical images, such as OCT. In OCT images, the retina is always
oriented in exactly the same direction, with the macula (the center of the
retina) usually located in roughly the same part of the image. This reduces the
complexity of training substantially, and we hypothesized that it might affect
the data requirements for learning on data such as these.

Nevertheless, given the two-alternative classification task performed here, this
finding is roughly consistent with the rule of thumb described by
\cite{Goodfellow-et-al-2016}(``... 5,000 labeled exemplars per category...'').
The sub-sampling method introduced here provides a protocol for researchers that
are interested in asking whether they have enough data to apply DL to their
biomedical image data.

Note that because there are 11 images used in each OCT volume, the $\sim$20,000
images represent approximately 1800 volumes of data, or 900 patients per group.
This number of patients is well within range for many traditional random
controlled trials and other clinical studies. This suggests that \emph{de novo}
training of DL networks could be integrated into many studies that are testing
new imaging technologies, or that are studying less common disorders.

\subsection{Other strategies}

There are currently two major alternative strategies to use for cases where data
is limited. Data augmentation synthetically increases the sample size by
performing transformations on the data \cite{Ciresan2011-ko}. This works well
as long as the transformations performed to do not destroy the information
necessary for classification, but introduce variability against which the DL
network should develop tolerance. For clinical imaging data, examples of such
transformations might be rigid translations and rotations of the image features.

The other strategy one might use when faced with limited data is transfer
learning. This strategy is based on the observation that DL algorithms trained
for different image processing tasks often learn very similar first-stage
filters \cite{Bengio2012-nh}. Therefore, in this approach learning begins with
one (larger) data set. This dataset may share only some limited similarity to
the datasets that are ultimately of interest, but this phase of learning allows
the network to converge on good enough front-end filters. Once learning in this
phase has converged, this network is then retrained on the dataset of interest.
This approach is quite promising, and has already been demonstrated to work
well in training a neural network in classification of images from OCT
\citep{Kermany2018-bq}

Both of these approaches are powerful complements to datasets that are not large
enough, but even before employing these strategies, practitioners might want to
assess whether the amount of data that they already have might be sufficient to
accurately learn the classification task at hand.

\subsection{Do we need a separate test set?}

Variance between different courses of training increased substantially with
reduced sample sizes. This variance is not due to sampling of different
individual items --- both average accuracy and variance in accuracy between
repetitions do not change substantially when the same items are repeatedly used
in different subsamples.

Given the limit on test-set accuracy, one might expect that cross-validation on
a separate test-set would be crucial, and that results in the test-set might
differ substantially from the best performance on a validation set
\cite{Zhang_2017}. Nevertheless, we found no systematic difference in
accuracy assessment on a separate test set relative to assessment of accuracy on
a validation set that is repeatedly used during training. This finding is
consistent with previous anecdotal evidence that has been mentioned in the
literature \cite{Krizhevsky2012-qc}.

An important limitation of this conclusion is that in the present study, 
the test set comes from the same pool of images and labels as the training 
and validation sets. In some cases, an independent test set can be acquired 
with a higher (``gold-standard'') level of accuracy. In these cases, it might
still be important to eventually evaluate the performance of the algorithm 
on a separate test set. 

Both of these findings may arise from the large number of parameters that are
fit through the DL algorithm. The learning procedure may thus converge to
different solutions based on initial conditions. When data is small, this may
result in divergent solutions, that depend on the randomly generated initial
conditions of the network. For larger networks, this implies that overfitting is
not induced through repeated use of a validation data-set in accuracy
assessments. However, further research would be needed to assess the limits of
this conclusion and to merit its broad application.
