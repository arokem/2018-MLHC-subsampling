@ARTICLE{Glorot2010-is,
  title   = "Understanding the difficulty of training deep feedforward neural
             networks",
  author  = "Glorot, Xavier and Bengio, Yoshua",
  journal = "Aistats",
  volume  =  9,
  year    =  2010
}


@ARTICLE{Kingma2014-jl,
  title         = "Adam: A Method for Stochastic Optimization",
  author        = "Kingma, Diederik and Ba, Jimmy",
  abstract      = "We introduce Adam, an algorithm for first-order
                   gradient-based optimization of stochastic objective
                   functions, based on adaptive estimates of lower-order
                   moments. The method is straightforward to implement, is
                   computationally efficient, has little memory requirements,
                   is invariant to diagonal rescaling of the gradients, and is
                   well suited for problems that are large in terms of data
                   and/or parameters. The method is also appropriate for
                   non-stationary objectives and problems with very noisy
                   and/or sparse gradients. The hyper-parameters have intuitive
                   interpretations and typically require little tuning. Some
                   connections to related algorithms, on which Adam was
                   inspired, are discussed. We also analyze the theoretical
                   convergence properties of the algorithm and provide a regret
                   bound on the convergence rate that is comparable to the best
                   known results under the online convex optimization
                   framework. Empirical results demonstrate that Adam works
                   well in practice and compares favorably to other stochastic
                   optimization methods. Finally, we discuss AdaMax, a variant
                   of Adam based on the infinity norm.",
  month         =  "22~" # dec,
  year          =  2014,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1412.6980"
}

@article{jia2014caffe,
  Author = {Jia, Yangqing and Shelhamer, Evan and Donahue, Jeff and Karayev, Sergey and Long, Jonathan and Girshick, Ross and Guadarrama, Sergio and Darrell, Trevor},
  Journal = {arXiv preprint arXiv:1408.5093},
  Title = {Caffe: Convolutional Architecture for Fast Feature Embedding},
  Year = {2014}
}

@article{Simonyan2014-al,
  author    = {Karen Simonyan and
               Andrew Zisserman},
  title     = {Very Deep Convolutional Networks for Large-Scale Image Recognition},
  journal   = {CoRR},
  volume    = {abs/1409.1556},
  year      = {2014},
  url       = {http://arxiv.org/abs/1409.1556},
  timestamp = {Wed, 01 Oct 2014 15:00:05 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/SimonyanZ14a},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@ARTICLE{LeCun2015-js,
  title       = "Deep learning",
  author      = "LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey",
  affiliation = "1] Facebook AI Research, 770 Broadway, New York, New York
                 10003 USA. [2] New York University, 715 Broadway, New York,
                 New York 10003, USA. Department of Computer Science and
                 Operations Research Universit{\'e} de Montr{\'e}al, Pavillon
                 Andr{\'e}-Aisenstadt, PO Box 6128 Centre-Ville STN
                 Montr{\'e}al, Quebec H3C 3J7, Canada. 1] Google, 1600
                 Amphitheatre Parkway, Mountain View, California 94043, USA.
                 [2] Department of Computer Science, University of Toronto, 6
                 King's College Road, Toronto, Ontario M5S 3G4, Canada.",
  abstract    = "Deep learning allows computational models that are composed of
                 multiple processing layers to learn representations of data
                 with multiple levels of abstraction. These methods have
                 dramatically improved the state-of-the-art in speech
                 recognition, visual object recognition, object detection and
                 many other domains such as drug discovery and genomics. Deep
                 learning discovers intricate structure in large data sets by
                 using the backpropagation algorithm to indicate how a machine
                 should change its internal parameters that are used to compute
                 the representation in each layer from the representation in
                 the previous layer. Deep convolutional nets have brought about
                 breakthroughs in processing images, video, speech and audio,
                 whereas recurrent nets have shone light on sequential data
                 such as text and speech.",
  journal     = "Nature",
  volume      =  521,
  number      =  7553,
  pages       = "436--444",
  month       =  "28~" # may,
  year        =  2015
}



@INCOLLECTION{Krizhevsky2012-az,
  title     = "{ImageNet} Classification with Deep Convolutional Neural
               Networks",
  booktitle = "Advances in Neural Information Processing Systems 25",
  author    = "Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E",
  editor    = "Pereira, F and Burges, C J C and Bottou, L and Weinberger, K Q",
  abstract  = "... We wrote a highly-optimized GPU implementation of 2D
               convolution and all the other ... Our results show that a large,
               deep convolutional neural network is capable of achieving
               record- breaking ... It is notable that our network's
               performance degrades if a single convolutional layer is ...",
  publisher = "Curran Associates, Inc.",
  pages     = "1097--1105",
  year      =  2012
}



@ARTICLE{Gulshan2016-fh,
  title       = "Development and Validation of a Deep Learning Algorithm for
                 Detection of Diabetic Retinopathy in Retinal Fundus
                 Photographs",
  author      = "Gulshan, Varun and Peng, Lily and Coram, Marc and Stumpe,
                 Martin C and Wu, Derek and Narayanaswamy, Arunachalam and
                 Venugopalan, Subhashini and Widner, Kasumi and Madams, Tom and
                 Cuadros, Jorge and Kim, Ramasamy and Raman, Rajiv and Nelson,
                 Philip C and Mega, Jessica L and Webster, Dale R",
  affiliation = "Google Inc, Mountain View, California. Google Inc, Mountain
                 View, California. Google Inc, Mountain View, California.
                 Google Inc, Mountain View, California. Google Inc, Mountain
                 View, California. Google Inc, Mountain View, California.
                 Google Inc, Mountain View, California2Department of Computer
                 Science, University of Texas, Austin. Google Inc, Mountain
                 View, California. Google Inc, Mountain View, California.
                 EyePACS LLC, San Jose, California4School of Optometry, Vision
                 Science Graduate Group, University of California, Berkeley.
                 Aravind Medical Research Foundation, Aravind Eye Care System,
                 Madurai, India. Shri Bhagwan Mahavir Vitreoretinal Services,
                 Sankara Nethralaya, Chennai, Tamil Nadu, India. Google Inc,
                 Mountain View, California. Verily Life Sciences, Mountain
                 View, California8Cardiovascular Division, Department of
                 Medicine, Brigham and Women's Hospital and Harvard Medical
                 School, Boston, Massachusetts. Google Inc, Mountain View,
                 California.",
  abstract    = "Importance: Deep learning is a family of computational methods
                 that allow an algorithm to program itself by learning from a
                 large set of examples that demonstrate the desired behavior,
                 removing the need to specify rules explicitly. Application of
                 these methods to medical imaging requires further assessment
                 and validation. Objective: To apply deep learning to create an
                 algorithm for automated detection of diabetic retinopathy and
                 diabetic macular edema in retinal fundus photographs. Design
                 and Setting: A specific type of neural network optimized for
                 image classification called a deep convolutional neural
                 network was trained using a retrospective development data set
                 of 128 175 retinal images, which were graded 3 to 7 times for
                 diabetic retinopathy, diabetic macular edema, and image
                 gradability by a panel of 54 US licensed ophthalmologists and
                 ophthalmology senior residents between May and December 2015.
                 The resultant algorithm was validated in January and February
                 2016 using 2 separate data sets, both graded by at least 7 US
                 board-certified ophthalmologists with high intragrader
                 consistency. Exposure: Deep learning-trained algorithm. Main
                 Outcomes and Measures: The sensitivity and specificity of the
                 algorithm for detecting referable diabetic retinopathy (RDR),
                 defined as moderate and worse diabetic retinopathy, referable
                 diabetic macular edema, or both, were generated based on the
                 reference standard of the majority decision of the
                 ophthalmologist panel. The algorithm was evaluated at 2
                 operating points selected from the development set, one
                 selected for high specificity and another for high
                 sensitivity. Results: The EyePACS-1 data set consisted of 9963
                 images from 4997 patients (mean age, 54.4 years; 62.2\% women;
                 prevalence of RDR, 683/8878 fully gradable images [7.8\%]);
                 the Messidor-2 data set had 1748 images from 874 patients
                 (mean age, 57.6 years; 42.6\% women; prevalence of RDR,
                 254/1745 fully gradable images [14.6\%]). For detecting RDR,
                 the algorithm had an area under the receiver operating curve
                 of 0.991 (95\% CI, 0.988-0.993) for EyePACS-1 and 0.990 (95\%
                 CI, 0.986-0.995) for Messidor-2. Using the first operating cut
                 point with high specificity, for EyePACS-1, the sensitivity
                 was 90.3\% (95\% CI, 87.5\%-92.7\%) and the specificity was
                 98.1\% (95\% CI, 97.8\%-98.5\%). For Messidor-2, the
                 sensitivity was 87.0\% (95\% CI, 81.1\%-91.0\%) and the
                 specificity was 98.5\% (95\% CI, 97.7\%-99.1\%). Using a
                 second operating point with high sensitivity in the
                 development set, for EyePACS-1 the sensitivity was 97.5\% and
                 specificity was 93.4\% and for Messidor-2 the sensitivity was
                 96.1\% and specificity was 93.9\%. Conclusions and Relevance:
                 In this evaluation of retinal fundus photographs from adults
                 with diabetes, an algorithm based on deep machine learning had
                 high sensitivity and specificity for detecting referable
                 diabetic retinopathy. Further research is necessary to
                 determine the feasibility of applying this algorithm in the
                 clinical setting and to determine whether use of the algorithm
                 could lead to improved care and outcomes compared with current
                 ophthalmologic assessment.",
  journal     = "JAMA",
  volume      =  316,
  number      =  22,
  pages       = "2402--2410",
  month       =  "13~" # dec,
  year        =  2016,
  language    = "en"
}



@ARTICLE{Esteva2017-ho,
  title       = "Dermatologist-level classification of skin cancer with deep
                 neural networks",
  author      = "Esteva, Andre and Kuprel, Brett and Novoa, Roberto A and Ko,
                 Justin and Swetter, Susan M and Blau, Helen M and Thrun,
                 Sebastian",
  affiliation = "Department of Electrical Engineering, Stanford University,
                 Stanford, California, USA. Department of Electrical
                 Engineering, Stanford University, Stanford, California, USA.
                 Department of Dermatology, Stanford University, Stanford,
                 California, USA. Department of Pathology, Stanford University,
                 Stanford, California, USA. Department of Dermatology, Stanford
                 University, Stanford, California, USA. Department of
                 Dermatology, Stanford University, Stanford, California, USA.
                 Dermatology Service, Veterans Affairs Palo Alto Health Care
                 System, Palo Alto, California, USA. Baxter Laboratory for Stem
                 Cell Biology, Department of Microbiology and Immunology,
                 Institute for Stem Cell Biology and Regenerative Medicine,
                 Stanford University, Stanford, California, USA. Department of
                 Computer Science, Stanford University, Stanford, California,
                 USA.",
  abstract    = "Skin cancer, the most common human malignancy, is primarily
                 diagnosed visually, beginning with an initial clinical
                 screening and followed potentially by dermoscopic analysis, a
                 biopsy and histopathological examination. Automated
                 classification of skin lesions using images is a challenging
                 task owing to the fine-grained variability in the appearance
                 of skin lesions. Deep convolutional neural networks (CNNs)
                 show potential for general and highly variable tasks across
                 many fine-grained object categories. Here we demonstrate
                 classification of skin lesions using a single CNN, trained
                 end-to-end from images directly, using only pixels and disease
                 labels as inputs. We train a CNN using a dataset of 129,450
                 clinical images-two orders of magnitude larger than previous
                 datasets-consisting of 2,032 different diseases. We test its
                 performance against 21 board-certified dermatologists on
                 biopsy-proven clinical images with two critical binary
                 classification use cases: keratinocyte carcinomas versus
                 benign seborrheic keratoses; and malignant melanomas versus
                 benign nevi. The first case represents the identification of
                 the most common cancers, the second represents the
                 identification of the deadliest skin cancer. The CNN achieves
                 performance on par with all tested experts across both tasks,
                 demonstrating an artificial intelligence capable of
                 classifying skin cancer with a level of competence comparable
                 to dermatologists. Outfitted with deep neural networks, mobile
                 devices can potentially extend the reach of dermatologists
                 outside of the clinic. It is projected that 6.3 billion
                 smartphone subscriptions will exist by the year 2021 (ref. 13)
                 and can therefore potentially provide low-cost universal
                 access to vital diagnostic care.",
  journal     = "Nature",
  volume      =  542,
  number      =  7639,
  pages       = "115--118",
  month       =  "2~" # feb,
  year        =  2017,
  language    = "en"
}


@article{lee2017deep,
  title={Deep Learning Is Effective for Classifying Normal versus Age-Related Macular Degeneration OCT Images},
  author={Lee, Cecilia S and Baughman, Doug M and Lee, Aaron Y},
  journal={Ophthalmology Retina},
  volume={1},
  number={4},
  pages={322--327},
  year={2017},
  publisher={Elsevier}
}
@ARTICLE{Cho2015data_size,
  title    = "How much data is needed to train a medical image deep learning
              system to achieve necessary high accuracy?",
  author   = "Cho, Junghwan and Lee, Kyewook and Shin, Ellie and Choy, Garry
              and Do, Synho",
  abstract = "The use of Convolutional Neural Networks (CNN) in natural image
              classification systems has produced very impressive results.
              Combined with the inherent nature of medical images that make
              them ideal for deep-learning, further application of such systems
              to medical image classification holds much promise. However, the
              usefulness and potential impact of such a system can be
              completely negated if it does not reach a target accuracy. In
              this paper, we present a study on determining the optimum size of
              the training data set necessary to achieve high classification
              accuracy with low variance in medical image classification
              systems. The CNN was applied to classify axial Computed
              Tomography (CT) images into six anatomical classes. We trained
              the CNN using six different sizes of training data set (5, 10,
              20, 50, 100, and 200) and then tested the resulting system with a
              total of 6000 CT images. All images were acquired from the
              Massachusetts General Hospital (MGH) Picture Archiving and
              Communication System (PACS). Using this data, we employ the
              learning curve approach to predict classification accuracy at a
              given training sample size. Our research will present a general
              methodology for determining the training data set size necessary
              to achieve a certain target classification accuracy that can be
              easily applied to other problems within such systems.",
  journal  = "arXiv [cs.LG]",
  month    =  "19~" # nov,
  year     =  2015
}


@book{Goodfellow-et-al-2016,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    note={\url{http://www.deeplearningbook.org}},
    year={2016}
}



@ARTICLE{Stone1977-ez,
  title     = "An Asymptotic Equivalence of Choice of Model by
               {Cross-Validation} and Akaike's Criterion",
  author    = "Stone, M",
  abstract  = "A logarithmic assessment of the performance of a predicting
               density is found to lead to asymptotic equivalence of choice of
               model by cross-validation and Akaike's criterion, when maximum
               likelihood estimation is used within each model.",
  journal   = "J. R. Stat. Soc. Series B Stat. Methodol.",
  publisher = "[Royal Statistical Society, Wiley]",
  volume    =  39,
  number    =  1,
  pages     = "44--47",
  year      =  1977
}

@INCOLLECTION{Karpathy2014-nx,
  title     = "Deep Fragment Embeddings for Bidirectional Image Sentence
               Mapping",
  booktitle = "Advances in Neural Information Processing Systems 27",
  author    = "Karpathy, Andrej and Joulin, Armand and Li, Fei Fei F",
  editor    = "Ghahramani, Z and Welling, M and Cortes, C and Lawrence, N D and
               Weinberger, K Q",
  abstract  = "... De- scribing the contents of images is useful for automated
               image captioning and conversely, the ability to retrieve images
               based on natural language queries has immediate ... JMLR (2003)
               [20] Ngiam, J., Khosla, A., Kim, M., Nam, J., Lee, H., Ng, AY:
               Multimodal deep learning . ...",
  publisher = "Curran Associates, Inc.",
  pages     = "1889--1897",
  year      =  2014
}



@ARTICLE{Donahue2014-uq,
  title         = "Long-term Recurrent Convolutional Networks for Visual
                   Recognition and Description",
  author        = "Donahue, Jeff and Hendricks, Lisa Anne and Rohrbach, Marcus
                   and Venugopalan, Subhashini and Guadarrama, Sergio and
                   Saenko, Kate and Darrell, Trevor",
  abstract      = "Models based on deep convolutional networks have dominated
                   recent image interpretation tasks; we investigate whether
                   models which are also recurrent, or ``temporally deep'', are
                   effective for tasks involving sequences, visual and
                   otherwise. We develop a novel recurrent convolutional
                   architecture suitable for large-scale visual learning which
                   is end-to-end trainable, and demonstrate the value of these
                   models on benchmark video recognition tasks, image
                   description and retrieval problems, and video narration
                   challenges. In contrast to current models which assume a
                   fixed spatio-temporal receptive field or simple temporal
                   averaging for sequential processing, recurrent convolutional
                   models are ``doubly deep''' in that they can be
                   compositional in spatial and temporal ``layers''. Such
                   models may have advantages when target concepts are complex
                   and/or training data are limited. Learning long-term
                   dependencies is possible when nonlinearities are
                   incorporated into the network state updates. Long-term RNN
                   models are appealing in that they directly can map
                   variable-length inputs (e.g., video frames) to variable
                   length outputs (e.g., natural language text) and can model
                   complex temporal dynamics; yet they can be optimized with
                   backpropagation. Our recurrent long-term models are directly
                   connected to modern visual convnet models and can be jointly
                   trained to simultaneously learn temporal dynamics and
                   convolutional perceptual representations. Our results show
                   such models have distinct advantages over state-of-the-art
                   models for recognition or generation which are separately
                   defined and/or optimized.",
  month         =  "17~" # nov,
  year          =  2014,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "1411.4389"
}



@ARTICLE{Hannun2014-uj,
  title         = "Deep Speech: Scaling up end-to-end speech recognition",
  author        = "Hannun, Awni and Case, Carl and Casper, Jared and Catanzaro,
                   Bryan and Diamos, Greg and Elsen, Erich and Prenger, Ryan
                   and Satheesh, Sanjeev and Sengupta, Shubho and Coates, Adam
                   and Ng, Andrew Y",
  abstract      = "We present a state-of-the-art speech recognition system
                   developed using end-to-end deep learning. Our architecture
                   is significantly simpler than traditional speech systems,
                   which rely on laboriously engineered processing pipelines;
                   these traditional systems also tend to perform poorly when
                   used in noisy environments. In contrast, our system does not
                   need hand-designed components to model background noise,
                   reverberation, or speaker variation, but instead directly
                   learns a function that is robust to such effects. We do not
                   need a phoneme dictionary, nor even the concept of a
                   ``phoneme.'' Key to our approach is a well-optimized RNN
                   training system that uses multiple GPUs, as well as a set of
                   novel data synthesis techniques that allow us to efficiently
                   obtain a large amount of varied data for training. Our
                   system, called Deep Speech, outperforms previously published
                   results on the widely studied Switchboard Hub5'00, achieving
                   16.0\% error on the full test set. Deep Speech also handles
                   challenging noisy environments better than widely used,
                   state-of-the-art commercial speech systems.",
  month         =  "17~" # dec,
  year          =  2014,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1412.5567"
}



@ARTICLE{Wu2016-kx,
  title         = "Google's Neural Machine Translation System: Bridging the Gap
                   between Human and Machine Translation",
  author        = "Wu, Yonghui and Schuster, Mike and Chen, Zhifeng and Le,
                   Quoc V and Norouzi, Mohammad and Macherey, Wolfgang and
                   Krikun, Maxim and Cao, Yuan and Gao, Qin and Macherey, Klaus
                   and Klingner, Jeff and Shah, Apurva and Johnson, Melvin and
                   Liu, Xiaobing and Kaiser, {\L}ukasz and Gouws, Stephan and
                   Kato, Yoshikiyo and Kudo, Taku and Kazawa, Hideto and
                   Stevens, Keith and Kurian, George and Patil, Nishant and
                   Wang, Wei and Young, Cliff and Smith, Jason and Riesa, Jason
                   and Rudnick, Alex and Vinyals, Oriol and Corrado, Greg and
                   Hughes, Macduff and Dean, Jeffrey",
  abstract      = "Neural Machine Translation (NMT) is an end-to-end learning
                   approach for automated translation, with the potential to
                   overcome many of the weaknesses of conventional phrase-based
                   translation systems. Unfortunately, NMT systems are known to
                   be computationally expensive both in training and in
                   translation inference. Also, most NMT systems have
                   difficulty with rare words. These issues have hindered NMT's
                   use in practical deployments and services, where both
                   accuracy and speed are essential. In this work, we present
                   GNMT, Google's Neural Machine Translation system, which
                   attempts to address many of these issues. Our model consists
                   of a deep LSTM network with 8 encoder and 8 decoder layers
                   using attention and residual connections. To improve
                   parallelism and therefore decrease training time, our
                   attention mechanism connects the bottom layer of the decoder
                   to the top layer of the encoder. To accelerate the final
                   translation speed, we employ low-precision arithmetic during
                   inference computations. To improve handling of rare words,
                   we divide words into a limited set of common sub-word units
                   (``wordpieces'') for both input and output. This method
                   provides a good balance between the flexibility of
                   ``character''-delimited models and the efficiency of
                   ``word''-delimited models, naturally handles translation of
                   rare words, and ultimately improves the overall accuracy of
                   the system. Our beam search technique employs a
                   length-normalization procedure and uses a coverage penalty,
                   which encourages generation of an output sentence that is
                   most likely to cover all the words in the source sentence.
                   On the WMT'14 English-to-French and English-to-German
                   benchmarks, GNMT achieves competitive results to
                   state-of-the-art. Using a human side-by-side evaluation on a
                   set of isolated simple sentences, it reduces translation
                   errors by an average of 60\% compared to Google's
                   phrase-based production system.",
  month         =  "26~" # sep,
  year          =  2016,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1609.08144"
}



@ARTICLE{Silver2016-uv,
  title   = "Mastering the game of Go with deep neural networks and tree search",
  author  = "Silver, David and Huang, Aja and Maddison, Chris J and Guez,
             Arthur and Sifre, Laurent and van den Driessche, George and
             Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam,
             Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and
             Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and
             Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and
             Graepel, Thore and Hassabis, Demis",
  journal = "Nature",
  volume  =  529,
  number  =  7587,
  pages   = "484--489",
  month   =  "27~" # jan,
  year    =  2016
}



@MISC{Hsu2017-cr,
  title        = "{AI} Decisively Defeats Human Poker Players",
  booktitle    = "{IEEE} Spectrum: Technology, Engineering, and Science News",
  author       = "Hsu, Jeremy",
  abstract     = "An AI named Libratus has beaten human pro players in no-limit
                  Texas Hold'em for the first time",
  month        =  "31~" # jan,
  year         =  2017,
  howpublished = "\url{http://spectrum.ieee.org/automaton/robotics/artificial-intelligence/ai-learns-from-mistakes-to-defeat-human-poker-players}",
  note         = "Accessed: 2017-3-28"
}


@ARTICLE{Stone1974-mo,
  title     = "{Cross-Validatory} Choice and Assessment of Statistical
               Predictions",
  author    = "Stone, M",
  abstract  = "A generalized form of the cross-validation criterion is applied
               to the choice and assessment of prediction using the
               data-analytic concept of a prescription. The examples used to
               illustrate the application are drawn from the problem areas of
               univariate estimation, linear regression and analysis of
               variance.",
  journal   = "J. R. Stat. Soc. Series B Stat. Methodol.",
  publisher = "[Royal Statistical Society, Wiley]",
  volume    =  36,
  number    =  2,
  pages     = "111--147",
  year      =  1974
}


@INCOLLECTION{Krizhevsky2012-qc,
  title     = "{ImageNet} Classification with Deep Convolutional Neural
               Networks",
  booktitle = "Advances in Neural Information Processing Systems 25",
  author    = "Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E",
  editor    = "Pereira, F and Burges, C J C and Bottou, L and Weinberger, K Q",
  publisher = "Curran Associates, Inc.",
  pages     = "1097--1105",
  year      =  2012
}


@ARTICLE{Efron2004-qe,
  title    = "The Estimation of Prediction Error",
  author   = "Efron, Bradley",
  abstract = "Having constructed a data-based estimation rule, perhaps a
              logistic regression or a classification tree, the statistician
              would like to know its performance as a predictor of future
              cases. There are two main theories concerning prediction error:
              (1) penalty methods such as Cp, Akaike's information criterion,
              and Stein's unbiased risk estimate that depend on the covariance
              between data points and their corresponding predictions; and (2)
              cross-validation and related nonparametric bootstrap techniques.
              This article concerns the connection between the two theories. A
              Rao--Blackwell type of relation is derived in which nonparametric
              methods such as cross-validation are seen to be randomized
              versions of their covariance penalty counterparts. The
              model-based penalty methods offer substantially better accuracy,
              assuming that the model is believable.",
  journal  = "J. Am. Stat. Assoc.",
  volume   =  99,
  number   =  467,
  pages    = "619--632",
  year     =  2004
}



@ARTICLE{Efron1983-vu,
  title     = "Estimating the Error Rate of a Prediction Rule: Improvement on
               {Cross-Validation}",
  author    = "Efron, Bradley",
  abstract  = "Abstract We construct a prediction rule on the basis of some
               data, and then wish to estimate the error rate of this rule in
               classifying future observations. Cross-validation provides a
               nearly unbiased estimate, using only the original data.
               Cross-validation turns out to be related closely to the
               bootstrap estimate of the error rate. This article has two
               purposes: to understand better the theoretical basis of the
               prediction problem, and to investigate some related estimators,
               which seem to offer considerably improved estimation in small
               samples.",
  journal   = "J. Am. Stat. Assoc.",
  publisher = "amstat.tandfonline.com",
  volume    =  78,
  number    =  382,
  pages     = "316--331",
  year      =  1983
}



@ARTICLE{Canziani2016-ps,
  title         = "An Analysis of Deep Neural Network Models for Practical
                   Applications",
  author        = "Canziani, Alfredo and Paszke, Adam and Culurciello, Eugenio",
  abstract      = "Since the emergence of Deep Neural Networks (DNNs) as a
                   prominent technique in the field of computer vision, the
                   ImageNet classification challenge has played a major role in
                   advancing the state-of-the-art. While accuracy figures have
                   steadily increased, the resource utilisation of winning
                   models has not been properly taken into account. In this
                   work, we present a comprehensive analysis of important
                   metrics in practical applications: accuracy, memory
                   footprint, parameters, operations count, inference time and
                   power consumption. Key findings are: (1) power consumption
                   is independent of batch size and architecture; (2) accuracy
                   and inference time are in a hyperbolic relationship; (3)
                   energy constraint is an upper bound on the maximum
                   achievable accuracy and model complexity; (4) the number of
                   operations is a reliable estimate of the inference time. We
                   believe our analysis provides a compelling set of
                   information that helps design and engineer efficient DNNs.",
  month         =  "24~" # may,
  year          =  2016,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "1605.07678"
}


@ARTICLE{Hinton2012-fm,
  title         = "Improving neural networks by preventing co-adaptation of
                   feature detectors",
  author        = "Hinton, Geoffrey E and Srivastava, Nitish and Krizhevsky,
                   Alex and Sutskever, Ilya and Salakhutdinov, Ruslan R",
  abstract      = "When a large feedforward neural network is trained on a
                   small training set, it typically performs poorly on held-out
                   test data. This ``overfitting'' is greatly reduced by
                   randomly omitting half of the feature detectors on each
                   training case. This prevents complex co-adaptations in which
                   a feature detector is only helpful in the context of several
                   other specific feature detectors. Instead, each neuron
                   learns to detect a feature that is generally helpful for
                   producing the correct answer given the combinatorially large
                   variety of internal contexts in which it must operate.
                   Random ``dropout'' gives big improvements on many benchmark
                   tasks and sets new records for speech and object
                   recognition.",
  month         =  "3~" # jul,
  year          =  2012,
  archivePrefix = "arXiv",
  primaryClass  = "cs.NE",
  eprint        = "1207.0580"
}




@ARTICLE{Glorot2011-hk,
  title   = "Deep Sparse Rectifier Neural Networks",
  author  = "Glorot, Xavier and Bordes, Antoine and Bengio, Yoshua",
  journal = "PMLR",
  volume  = "15:",
  pages   = "315--323",
  year    =  2011
}




@ARTICLE{Ciresan2011-ko,
  title         = "{High-Performance} Neural Networks for Visual Object
                   Classification",
  author        = "Cire{\c s}an, Dan C and Meier, Ueli and Masci, Jonathan and
                   Gambardella, Luca M and Schmidhuber, J{\"u}rgen",
  abstract      = "We present a fast, fully parameterizable GPU implementation
                   of Convolutional Neural Network variants. Our feature
                   extractors are neither carefully designed nor pre-wired, but
                   rather learned in a supervised way. Our deep hierarchical
                   architectures achieve the best published results on
                   benchmarks for object classification (NORB, CIFAR10) and
                   handwritten digit recognition (MNIST), with error rates of
                   2.53\%, 19.51\%, 0.35\%, respectively. Deep nets trained by
                   simple back-propagation perform better than more shallow
                   ones. Learning is surprisingly rapid. NORB is completely
                   trained within five epochs. Test error rates on MNIST drop
                   to 2.42\%, 0.97\% and 0.48\% after 1, 3 and 17 epochs,
                   respectively.",
  month         =  "1~" # feb,
  year          =  2011,
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI",
  eprint        = "1102.0183"
}



@ARTICLE{Lemaitre2016-gu,
  title     = "Classification of {SD-OCT} Volumes Using Local Binary Patterns:
               Experimental Validation for {DME} Detection",
  author    = "Lema\^{i}tre, Guillaume and Rastgoo, Mojdeh and
               Massich, Joan and Cheung, Carol Y and Wong, Tien Y and
               Lamoureux, Ecosse and Milea, Dan and M\`{e}riaudeau,
               Fabrice and Sidib\`{e}, D\`{e}sir\`{e}",
  abstract  = "This paper addresses the problem of automatic classification of
               Spectral Domain OCT (SD-OCT) data for automatic identification
               of patients with DME versus normal subjects. Optical Coherence
               Tomography (OCT) has been a valuable diagnostic tool for DME,
               which is among the most common causes of irreversible vision
               loss in individuals with diabetes. Here, a classification
               framework with five distinctive steps is proposed and we present
               an extensive study of each step. Our method considers
               combination of various preprocessing steps in conjunction with
               Local Binary Patterns (LBP) features and different mapping
               strategies. Using linear and nonlinear classifiers, we tested
               the developed framework on a balanced cohort of 32 patients.
               Experimental results show that the proposed method outperforms
               the previous studies by achieving a Sensitivity (SE) and a
               Specificity (SP) of 81.2\% and 93.7\%, respectively. Our study
               concludes that the 3D features and high-level representation of
               2D features using patches achieve the best results. However, the
               effects of preprocessing are inconsistent with different
               classifiers and feature configurations.",
  journal   = "J. Ophthalmol.",
  publisher = "Hindawi Publishing Corporation",
  volume    =  2016,
  month     =  "31~" # jul,
  year      =  2016,
  language  = "en"
}



@ARTICLE{Bengio2012-nh,
  title     = "Deep learning of representations for unsupervised and transfer
               learning",
  author    = "Bengio, Yoshua and {Others}",
  abstract  = "... that representations of X that are useful to capture P(X)
               are also in part useful to capture P(Y |X). In the context of
               the Unsupervised and Transfer Learning Challenge, the assumption
               exploited by Deep Learning algorithms goes even further, and is
               related to the Self-Taught 21 ...",
  journal   = "ICML Unsupervised and Transfer Learning",
  publisher = "jmlr.org",
  volume    =  27,
  pages     = "17--36",
  year      =  2012
}



@ARTICLE{Zhang_2017,
  title   = "Understanding Deep Learning Requires Rethinking Generalization",
  author  = "Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht,
             Benjamin and Vinyals, Oriol",
  journal = "ICLR 2017",
  year    = 2017
}



@ARTICLE{Lee2017-vp,
  title       = "Deep-learning based, automated segmentation of macular edema
                 in optical coherence tomography",
  author      = "Lee, Cecilia S and Tyring, Ariel J and Deruyter, Nicolaas P
                 and Wu, Yue and Rokem, Ariel and Lee, Aaron Y",
  affiliation = "Department of Ophthalmology, University of Washington,
                 Seattle, Washington, USA. University of Washington School of
                 Medicine, Seattle, Washington, USA. eScience Institute,
                 University of Washington, Seattle, Washington, USA. Department
                 of Ophthalmology, Puget Sound Veteran Affairs, Seattle
                 Washington, USA.",
  abstract    = "Evaluation of clinical images is essential for diagnosis in
                 many specialties. Therefore the development of computer vision
                 algorithms to help analyze biomedical images will be
                 important. In ophthalmology, optical coherence tomography
                 (OCT) is critical for managing retinal conditions. We
                 developed a convolutional neural network (CNN) that detects
                 intraretinal fluid (IRF) on OCT in a manner indistinguishable
                 from clinicians. Using 1,289 OCT images, the CNN segmented
                 images with a 0.911 cross-validated Dice coefficient, compared
                 with segmentations by experts. Additionally, the agreement
                 between experts and between experts and CNN were similar. Our
                 results reveal that CNN can be trained to perform automated
                 segmentations of clinically relevant image features.",
  journal     = "Biomed. Opt. Express",
  volume      =  8,
  number      =  7,
  pages       = "3440--3448",
  month       =  jul,
  year        =  2017,
  keywords    = "(110.4500) Optical coherence tomography; (150.1135) Algorithms",
  language    = "en"
}



@INPROCEEDINGS{Awais2017-qw,
  title     = "Classification of {SD-OCT} images using a Deep learning approach",
  booktitle = "2017 {IEEE} International Conference on Signal and Image
               Processing Applications ({ICSIPA})",
  author    = "Awais, M and M{\"u}ller, H and Tang, T B and Meriaudeau, F",
  abstract  = "Diabetic Macular Edema (DME) is one of the many eye diseases
               that is commonly found in diabetic patients. If it is left
               untreated it may cause vision loss. This paper focuses on
               classification of abnormal and normal OCT (Optical Coherence
               Tomography) image volumes using a pre-trained CNN (Convolutional
               Neural Network). Using VGG16 (Visual Geometry Group), features
               are extracted at different layers of the network, e.g. before
               fully connected layer and after each fully connected layer. On
               the basis of these features classification was performed using
               different classifiers and results are higher than recently
               published work on the same dataset with an accuracy of 87.5\%,
               with sensitivity and specificity being 93.5\% and 81\%
               respectively.",
  pages     = "489--492",
  year      =  2017,
  keywords  = "biomedical optical imaging;diseases;eye;image
               classification;learning (artificial intelligence);medical image
               processing;neural nets;optical tomography;DME;Diabetic Macular
               Edema;SD-OCT images;VGG16;convolutional neural network;deep
               learning approach;diabetic patients;eye diseases;optical
               coherence tomography;pre-trained CNN;vision loss;visual geometry
               group;Biomedical imaging;Convolution;Feature extraction;Machine
               learning;Optical coherence tomography;Retina;Deep
               learning;Diabetic Macular Edema (DME);Feature Matrices;Visual
               Graphic Geometry (VGG)"
}



@ARTICLE{Kermany2018-bq,
  title     = "Identifying Medical Diagnoses and Treatable Diseases by
               {Image-Based} Deep Learning",
  author    = "Kermany, Daniel S and Goldbaum, Michael and Cai, Wenjia and
               Valentim, Carolina C S and Liang, Huiying and Baxter, Sally L
               and McKeown, Alex and Yang, Ge and Wu, Xiaokang and Yan,
               Fangbing and Dong, Justin and Prasadha, Made K and Pei,
               Jacqueline and Ting, Magdalena and Zhu, Jie and Li, Christina
               and Hewett, Sierra and Dong, Jason and Ziyar, Ian and Shi,
               Alexander and Zhang, Runze and Zheng, Lianghong and Hou, Rui and
               Shi, William and Fu, Xin and Duan, Yaou and Huu, Viet A N and
               Wen, Cindy and Zhang, Edward D and Zhang, Charlotte L and Li,
               Oulan and Wang, Xiaobo and Singer, Michael A and Sun, Xiaodong
               and Xu, Jie and Tafreshi, Ali and Anthony Lewis, M and Xia,
               Huimin and Zhang, Kang",
  abstract  = "SummaryThe implementation of clinical-decision support
               algorithms for medical imaging faces challenges with reliability
               and interpretability. Here, we establish a diagnostic tool based
               on a deep-learning framework for the screening of patients with
               common treatable blinding retinal diseases. Our framework
               utilizes transfer learning, which trains a neural network with a
               fraction of the data of conventional approaches. Applying this
               approach to a dataset of optical coherence tomography images, we
               demonstrate performance comparable to that of human experts in
               classifying age-related macular degeneration and diabetic
               macular edema. We also provide a more transparent and
               interpretable diagnosis by highlighting the regions recognized
               by the neural network. We further demonstrate the general
               applicability of our AI system for diagnosis of pediatric
               pneumonia using chest X-ray images. This tool may ultimately aid
               in expediting the diagnosis and referral of these treatable
               conditions, thereby facilitating earlier treatment, resulting in
               improved clinical outcomes.Video Abstract",
  journal   = "Cell",
  publisher = "Elsevier",
  volume    =  172,
  number    =  5,
  pages     = "1122--1131.e9",
  month     =  feb,
  year      =  2018,
  keywords  = "artificial intelligence; transfer learning; deep learning;
               age-related macular degeneration; choroidal neovascularization;
               diabetic retinopathy; diabetic macular edema; screening; optical
               coherence tomography; pneumonia",
  language  = "en"
}



@ARTICLE{Rajpurkar2017-mv,
  title         = "{CheXNet}: {Radiologist-Level} Pneumonia Detection on Chest
                   {X-Rays} with Deep Learning",
  author        = "Rajpurkar, Pranav and Irvin, Jeremy and Zhu, Kaylie and
                   Yang, Brandon and Mehta, Hershel and Duan, Tony and Ding,
                   Daisy and Bagul, Aarti and Langlotz, Curtis and Shpanskaya,
                   Katie and Lungren, Matthew P and Ng, Andrew Y",
  abstract      = "We develop an algorithm that can detect pneumonia from chest
                   X-rays at a level exceeding practicing radiologists. Our
                   algorithm, CheXNet, is a 121-layer convolutional neural
                   network trained on ChestX-ray14, currently the largest
                   publicly available chest X-ray dataset, containing over
                   100,000 frontal-view X-ray images with 14 diseases. Four
                   practicing academic radiologists annotate a test set, on
                   which we compare the performance of CheXNet to that of
                   radiologists. We find that CheXNet exceeds average
                   radiologist performance on the F1 metric. We extend CheXNet
                   to detect all 14 diseases in ChestX-ray14 and achieve state
                   of the art results on all 14 diseases.",
  month         =  nov,
  year          =  2017,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "1711.05225"
}
